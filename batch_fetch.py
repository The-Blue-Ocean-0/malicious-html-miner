import pandas as pd
import asyncio
import time
import json
import os
import re
import logging
from playwright.async_api import async_playwright

# ------------------- ì„¤ì • -------------------
RESULT_DIR = 'data/results/batch_results'     # ãƒãƒƒãƒçµæœãƒ•ã‚©ãƒ«ãƒ€
LOG_DIR = 'data/log'                          # ãƒ­ã‚°ä¿å­˜ãƒ•ã‚©ãƒ«ãƒ€
EXEC_FILE = 'urls_data.csv'                   # å®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ«
SCREENSHOT_DIR = 'data/screenshots'           # ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆä¿å­˜å…ˆ
ENABLE_SCREENSHOT = False
BATCH_SIZE = 20                               # 1ãƒãƒƒãƒã”ã¨ã®URLæ•°   
MAX_CONCURRENT = 5                            # åŒæ™‚æ¥ç¶šæœ€å¤§æ•°
NONE_DATA = 'null'
# --------------------------------------------

os.makedirs(SCREENSHOT_DIR, exist_ok=True)
os.makedirs(RESULT_DIR, exist_ok=True)
os.makedirs(LOG_DIR, exist_ok=True)

# ãƒ­ã‚¬ãƒ¼ã‚’ãƒãƒƒãƒã”ã¨ã«è¨­å®š
def setup_logger(batch_num):
    logger = logging.getLogger(f"batch_{batch_num}")
    logger.setLevel(logging.INFO)

    if not logger.handlers:
        log_path = f"{LOG_DIR}/batch_{batch_num}.log"
        handler = logging.FileHandler(log_path, encoding='utf-8')
        formatter = logging.Formatter('%(asctime)s [%(levelname)s] %(message)s')
        handler.setFormatter(formatter)
        logger.addHandler(handler)

        # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«å‡ºåŠ›ã‚‚è¿½åŠ ï¼ˆä»»æ„ï¼‰
        console = logging.StreamHandler()
        console.setFormatter(formatter)
        logger.addHandler(console)

    return logger

async def extract_head_elements(url, batch_index, batch_total, logger, batch_num):
    result = {
        "original_url": url,
        "final_url": None,
        "redirect_chain": None,
        "redirect_count": 0,
        "head_elements": None,
        "timeout": False,
        "has_meta_refresh": False,
        "meta_refresh_url": None,
        "duration_sec": None,
    }

    logger.info(f"[BATCH {batch_num}][{batch_index}/{batch_total}] â–¶ï¸ Start: {url}")
    start_time = time.time()

    try:
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page(user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")

            redirect_chain = []
            page.on("response", lambda response: (
                redirect_chain.append({
                    "url": response.url,
                    "status": response.status,
                    "location": response.headers.get("location", "")
                }) if response.status in [301, 302, 303, 307, 308] else None
            ))

            try:
                await page.goto(url, timeout=10000, wait_until='domcontentloaded')
                result["final_url"] = page.url
                result["redirect_chain"] = (
                    None if len(redirect_chain) == 0 else json.dumps(redirect_chain, ensure_ascii=False)
                )
                result["redirect_count"] = len(redirect_chain)

                html = await page.content()

                match = re.search(r'<meta[^>]+http-equiv=["\']refresh["\'][^>]+content=["\']\s*\d+\s*;\s*url=([^"\']+)["\']', html, re.IGNORECASE)
                if match:
                    result["has_meta_refresh"] = True
                    result["meta_refresh_url"] = match.group(1).strip()

                elements = []
                head_children = await page.query_selector_all("head > *")
                for tag in head_children:
                    tag_name = await tag.evaluate("(el) => el.tagName.toLowerCase()")
                    if tag_name in ["style", "script"]:
                        continue
                    attributes = await tag.evaluate("(el) => Object.fromEntries([...el.attributes].map(attr => [attr.name, attr.value]))")
                    text = await tag.inner_text()
                    elements.append({
                        "tag": tag_name,
                        "attributes": attributes,
                        "text": text.strip() if text else ""
                    })

                if elements:
                    result["head_elements"] = elements
                else:
                    if ENABLE_SCREENSHOT:
                        await page.screenshot(path=f"{SCREENSHOT_DIR}/{batch_index+1}.png")

            except Exception as e:
                logger.warning(f"[BATCH {batch_num}][{batch_index}/{batch_total}] âš ï¸ Error loading {url}: {e}")
                if "Timeout" in str(e):
                    result["timeout"] = True
                if ENABLE_SCREENSHOT:
                    await page.screenshot(path=f"{SCREENSHOT_DIR}/{batch_index+1}.png")
            finally:
                await browser.close()
    except Exception as e:
        logger.error(f"[BATCH {batch_num}][{batch_index}/{batch_total}] âŒ Critical error for {url}: {e}")
        result["timeout"] = True

    result["duration_sec"] = round(time.time() - start_time, 2)
    logger.info(f"[BATCH {batch_num}][{batch_index}/{batch_total}] âœ… Done in {result['duration_sec']}s\n")
    return result

async def process_urls_batch(urls, batch_start_index, batch_num):
    sem = asyncio.Semaphore(MAX_CONCURRENT)
    batch_total = len(urls)
    logger = setup_logger(batch_num)

    async def extract_with_limit(url, batch_index):
        async with sem:
            return await extract_head_elements(url, batch_index, batch_total, logger, batch_num)

    tasks = [
        extract_with_limit(
            "http://" + url if not url.startswith("http") else url,
            idx  # ãƒãƒƒãƒå†…ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆ0ï½BATCH_SIZE-1ï¼‰
        )
        for idx, url in enumerate(urls)
    ]
    return await asyncio.gather(*tasks)

# ---------------- ì‹¤í–‰ë¶€ ----------------
df = pd.read_csv(EXEC_FILE, low_memory=False)
urls = df["original_url"].dropna().tolist()
total_urls = len(urls)

all_result_paths = []

for batch_idx in range(0, total_urls, BATCH_SIZE):
    batch_urls = urls[batch_idx:batch_idx + BATCH_SIZE]
    batch_num = batch_idx // BATCH_SIZE + 1
    print(f"\nğŸš€ Batch {batch_num}: Processing {len(batch_urls)} URLs...")

    results = asyncio.run(process_urls_batch(batch_urls, batch_idx, batch_num))
    results_df = pd.DataFrame(results)

    result_file = f"{RESULT_DIR}/batch_{batch_num}.csv"
    results_df.to_csv(result_file, index=False, na_rep=NONE_DATA)
    all_result_paths.append(result_file)

# çµæœçµ±åˆ
print("\nğŸ§© Merging all batch results...")
all_dfs = [pd.read_csv(path) for path in all_result_paths]
final_df = pd.concat(all_dfs, ignore_index=True)
final_df.to_csv("data/results/head_extraction_results.csv", index=False, na_rep=NONE_DATA)

print("ğŸ‰ å…¨ãƒãƒƒãƒå‡¦ç†å®Œäº†ï¼çµæœã¯ 'data/results/head_extraction_results.csv' ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸã€‚")
